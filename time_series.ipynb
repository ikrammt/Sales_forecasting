{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sales Forecasting for Toys  \n",
    "\n",
    "**Objective**: Predict daily product sales across stores data to optimize stock levels and reduce costs.  \n",
    "\n",
    "**Approach**:  \n",
    "1. **Clean and merge** transactional data.  \n",
    "2. **Explore patterns** (seasonality, top-selling categories).  \n",
    "3. **Engineer features** (lags, inventory flags) for machine learning.  \n",
    "4. **Train Random Forest & XGBoost** and explain predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\"data/sales.csv\")\n",
    "products = pd.read_csv(\"data/products.csv\")\n",
    "stores = pd.read_csv(\"data/stores.csv\")\n",
    "inventory = pd.read_csv(\"data/inventory.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data  \n",
    "Handle missing values, duplicates, and formatting:  \n",
    "- **Sales**: Drop rows with missing `Product_ID` or negative `Units`.  \n",
    "- **Products**: Convert prices from `$X.XX` to numeric.  \n",
    "- **Inventory**: Flag stockouts (`Stock_On_Hand = 0`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(sales, products, stores, inventory):\n",
    "    # Drop rows with critical missing values\n",
    "    sales.dropna(subset=['Product_ID', 'Store_ID', 'Units', 'Date'], inplace=True)\n",
    "    products.dropna(subset=['Product_ID', 'Product_Price', 'Product_Cost'], inplace=True)\n",
    "    stores.dropna(subset=['Store_ID'], inplace=True)\n",
    "\n",
    "    # Add duplicate check\n",
    "    sales.drop_duplicates(subset=['Sale_ID'], inplace=True)\n",
    "\n",
    "    # Clean and convert price columns early\n",
    "    products['Product_Cost'] = products['Product_Cost'].astype(str).str.replace('$', '').str.replace(' ', '')\n",
    "    products['Product_Price'] = products['Product_Price'].astype(str).str.replace('$', '').str.replace(' ', '')\n",
    "    products['Product_Cost'] = pd.to_numeric(products['Product_Cost'], errors='coerce')\n",
    "    products['Product_Price'] = pd.to_numeric(products['Product_Price'], errors='coerce')\n",
    "\n",
    "    # Convert date columns\n",
    "    sales['Date'] = pd.to_datetime(sales['Date'], errors='coerce')\n",
    "    sales.dropna(subset=['Date'], inplace=True)\n",
    "\n",
    "    # Remove invalid values\n",
    "    sales = sales[sales['Units'] >= 0]\n",
    "    products = products[products['Product_Price'] > 0]\n",
    "    products = products[products['Product_Cost'] >= 0]\n",
    "\n",
    "    # Clean inventory \n",
    "    inventory.dropna(subset=['Store_ID', 'Product_ID'], inplace=True)\n",
    "    inventory['Stock_On_Hand'] = pd.to_numeric(inventory['Stock_On_Hand'], errors='coerce')\n",
    "    inventory = inventory[inventory['Stock_On_Hand'] >= 0]\n",
    "\n",
    "    return sales, products, stores, inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales, products, stores, inventory = clean_data(sales, products, stores, inventory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_prepare(sales, products, stores):\n",
    "    merged = sales.merge(products, on=\"Product_ID\").merge(stores, on=\"Store_ID\")\n",
    "\n",
    "    # Create new columns for total sales and total cost\n",
    "    merged['sales'] = merged['Units'] * merged['Product_Price']\n",
    "    merged['cost'] = merged['Units'] * merged['Product_Cost']\n",
    "    merged['Date'] = pd.to_datetime(merged['Date'])\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merge_and_prepare(sales, products, stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering  \n",
    "Create time-based and aggregate features:  \n",
    "- **Time Features**: `Day_of_Week`, `Month`, `Is_Weekend`.  \n",
    "- **Lag Features**: `Lag_1_Day`, `Lag_7_Day` (previous sales).  \n",
    "- **Inventory Signals**: `Stockout_Flag`, `Inventory_Turnover`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(merged):\n",
    "    # Time features\n",
    "    merged['Day_of_Week'] = merged['Date'].dt.dayofweek\n",
    "    merged['Month'] = merged['Date'].dt.month\n",
    "    merged['Is_Weekend'] = (merged['Day_of_Week'] >= 5).astype(int)\n",
    "    \n",
    "    # Lag features (sort first!)\n",
    "    merged = merged.sort_values(['Store_ID', 'Product_ID', 'Date'])\n",
    "    merged['Lag_1_Day'] = merged.groupby(['Store_ID', 'Product_ID'])['Units'].shift(1)\n",
    "    merged['Lag_7_Day'] = merged.groupby(['Store_ID', 'Product_ID'])['Units'].shift(7)\n",
    "    \n",
    "    #Handle NaNs\n",
    "    merged['Lag_1_Day'] = merged['Lag_1_Day'].fillna(0)  # No prior data = assume 0\n",
    "    merged['Lag_7_Day'] = merged['Lag_7_Day'].fillna(0)\n",
    "\n",
    "    # Rolling stats\n",
    "    merged['Rolling_Avg_7'] = merged.groupby(['Store_ID', 'Product_ID'])['Units'].transform(\n",
    "        lambda x: x.rolling(7, min_periods=1).mean()\n",
    "    )\n",
    "    merged['Rolling_Std_7'] = (\n",
    "    merged.groupby(['Store_ID', 'Product_ID'])['Units']\n",
    "    .transform(lambda x: x.rolling(7, min_periods=1).std())\n",
    "    )\n",
    "    \n",
    "    # Product/store aggregates\n",
    "    # Avg sales per product category\n",
    "    merged['Category_Avg_Sales'] = merged.groupby('Product_Category')['Units'].transform('mean')\n",
    "\n",
    "    # Store performance (percentile rank)\n",
    "    merged['Store_Percentile'] = merged.groupby('Store_ID')['sales'].rank(pct=True)\n",
    "\n",
    "    # Sales-Based Stockout Proxies\n",
    "    # Flag sudden sales drops to zero\n",
    "    prev_day_sales = merged.groupby(['Store_ID', 'Product_ID'])['Units'].shift(1)\n",
    "    merged['Sales_Drop_Flag'] = ((merged['Units'] == 0) & (prev_day_sales > 0)).astype(int)\n",
    "\n",
    "    # Track consecutive days with zero sales\n",
    "    merged['Consecutive_Zero_Sales'] = (\n",
    "        merged.groupby(['Store_ID', 'Product_ID'])['Units']\n",
    "        .transform(lambda x: x.groupby((x != 0).cumsum()).cumcount())\n",
    "    )\n",
    "\n",
    "    # Price Elasticity Features\n",
    "    merged['Price_Change'] = merged.groupby('Product_ID')['Product_Price'].diff()\n",
    "    merged['Price_Increase_Flag'] = (merged['Price_Change'] > 0).astype(int)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = engineer_features(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eda(merged):\n",
    "    print(\"\\n--- Descriptive Statistics ---\")\n",
    "    print(merged.describe())\n",
    "    print(\"\\n--- Missing Values ---\")\n",
    "    print(merged.isnull().sum())\n",
    "    #Top 3 cities by sales\n",
    "    top_cities = merged.groupby('Store_City')['sales'].sum().sort_values(ascending=False).nlargest(3)\n",
    "    print(\"\\n---Top 3 Cities by Sales ---\")\n",
    "    print(top_cities)\n",
    "\n",
    "    #Seasonality Check\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_acf(merged.groupby('Date')['Units'].sum(), lags=30)\n",
    "    plt.title(\"Autocorrelation (Lags 1-30)\")\n",
    "    plt.show()\n",
    "    \n",
    "    #Store-Category Heatmap\n",
    "    sales_pivot = merged.pivot_table(index='Store_City', columns='Product_Category', values='Units', aggfunc='sum')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(sales_pivot.fillna(0), cmap='viridis', annot=True, fmt='.0f')\n",
    "    plt.title(\"Units Sold by Store City & Product Category\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(merged['Units'], bins=30, kde=True)\n",
    "    plt.title(\"Distribution of Units Sold\")\n",
    "    plt.xlabel(\"Units\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"Product_Category\", y=\"Units\", data=merged)\n",
    "    plt.title(\"Units Sold by Product Category\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Profit Trend Visualization ---\n",
    "    merged['profit'] = merged['sales'] - merged['cost']\n",
    "    profit_trend = merged.groupby('Date')['profit'].sum().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=profit_trend, x='Date', y='profit')\n",
    "    plt.title(\"Daily Profit Trend\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Profit ($)\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Daily vs Weekly vs Monthly Mean Sales ---\n",
    "    sales_daily = merged.groupby('Date')['sales'].sum()\n",
    "    sales_daily_df = pd.DataFrame(sales_daily)\n",
    "    sales_weekly_mean = sales_daily_df['sales'].resample('W').mean()\n",
    "    sales_monthly_mean = sales_daily_df['sales'].resample('M').mean()\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(sales_daily_df['sales'], label='Daily')\n",
    "    plt.plot(sales_weekly_mean, label='Weekly Mean')\n",
    "    plt.plot(sales_monthly_mean, label='Monthly Mean')\n",
    "    plt.title(\"Sales Resampling: Daily vs Weekly vs Monthly Mean\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sales ($)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eda(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "To forecast daily sales at the product-store level, we implemented **Random Forest Regressor** & **XGBoost**— powerful ensemble learning method known for its robustness and ability to handle complex patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "We selected key features that capture temporal trends, store dynamics, and product behavior:\n",
    "\n",
    "- **Temporal features**: `Day_of_Week`, `Month`, `Is_Weekend`\n",
    "- **Lag features**: `Lag_1_Day`, `Lag_7_Day`\n",
    "- **Rolling statistics**: `Rolling_Avg_7`\n",
    "- **Sales behavior flags**: `Sales_Drop_Flag`, `Consecutive_Zero_Sales`\n",
    "- **Category & store dynamics**: `Category_Avg_Sales`, `Store_Percentile`\n",
    "\n",
    "#### Chronological Train-Test Split\n",
    "\n",
    "To preserve the time series nature of the data:\n",
    "\n",
    "- The dataset was sorted by `Date`.\n",
    "- The last 20% of the timeline was held out as a **test set** to evaluate the model’s ability to predict unseen future sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    'Day_of_Week', 'Month', 'Is_Weekend',\n",
    "    'Lag_1_Day', 'Lag_7_Day', 'Rolling_Avg_7',\n",
    "    'Sales_Drop_Flag', 'Consecutive_Zero_Sales', \n",
    "    'Category_Avg_Sales', 'Store_Percentile'\n",
    "]\n",
    "target = 'Units'\n",
    "\n",
    "# Sort by date first to ensure chronological order\n",
    "merged_data = merged_data.sort_values('Date')\n",
    "\n",
    "# Define split point (e.g., last 20% of dates for test)\n",
    "split_date = merged_data['Date'].quantile(0.8, interpolation='nearest')\n",
    "print(f\"Splitting at: {split_date}\")  \n",
    "\n",
    "train = merged_data[merged_data['Date'] < split_date]\n",
    "test = merged_data[merged_data['Date'] >= split_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify split sizes\n",
    "print(f\"Train dates: {train['Date'].min()} to {train['Date'].max()}\")\n",
    "print(f\"Test dates: {test['Date'].min()} to {test['Date'].max()}\")\n",
    "print(f\"Train size: {len(train)}, Test size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "A **Random Forest Regressor** was trained using the training set:\n",
    "\n",
    "- `n_estimators=200`: Number of decision trees\n",
    "- `max_depth=10`: Limits the depth of each tree to reduce overfitting\n",
    "- `random_state=42`: Ensures reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "model.fit(train[features], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "We've added XGBoost (Extreme Gradient Boosting) as a second predictive model because:\n",
    "- It often provides superior performance for structured/tabular data\n",
    "- Handles non-linear relationships well\n",
    "- Automatically learns feature interactions\n",
    "- Provides built-in regularization to prevent overfitting\n",
    "\n",
    "Key parameters configured:\n",
    "- `n_estimators=200`: Number of boosting rounds\n",
    "- `max_depth=6`: Maximum tree depth\n",
    "- `learning_rate=0.1`: Step size shrinkage\n",
    "- `subsample=0.8`: Fraction of samples used per tree\n",
    "- `colsample_bytree=0.8`: Fraction of features used per tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model Training\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Evaluation\n",
    "\n",
    "The model’s predictions were evaluated using the following metrics:\n",
    "\n",
    "- **MAE (Mean Absolute Error)**: Measures the average size of the errors in units, treating all errors equally regardless of their direction.\n",
    "- **RMSE (Root Mean Squared Error)**: Measures the average magnitude of the errors, penalizing larger errors more heavily. Lower RMSE indicates better model performance.\n",
    "- **R² Score (Coefficient of Determination)**: Indicates how well your model explains the variability in the target variable—in this case, sales in units. A higher R² means the model captures more of the variation in the data.\n",
    "- **Naive Baseline**: Uses `Lag_1_Day` (sales from the previous day) as a simple benchmark to compare model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(test[target], predictions)\n",
    "rmse = np.sqrt(mean_squared_error(test[target], predictions))\n",
    "r2 = r2_score(test[target], predictions)\n",
    "\n",
    "print(f\"\\nRandom Forest Performance:\")\n",
    "print(f\"MAE: {mae:.2f} units\")\n",
    "print(f\"RMSE: {rmse:.2f} units\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Naive baseline (Lag-1)\n",
    "naive_mae = mean_absolute_error(test[target], test['Lag_1_Day'])\n",
    "print(f\"Naive MAE (Lag-1 baseline): {naive_mae:.2f} units\")\n",
    "\n",
    "# Check if model beats baseline\n",
    "improvement = (naive_mae - mae) / naive_mae * 100\n",
    "print(f\"Model improvement over naive: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost on the same training data\n",
    "xgb_model.fit(train[features], train[target])\n",
    "\n",
    "# Make predictions\n",
    "xgb_predictions = xgb_model.predict(test[features])\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_mae = mean_absolute_error(test[target], xgb_predictions)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(test[target], xgb_predictions))\n",
    "xgb_r2 = r2_score(test[target], xgb_predictions)\n",
    "\n",
    "# Compare with naive baseline\n",
    "xgb_improvement = (naive_mae - xgb_mae) / naive_mae * 100\n",
    "\n",
    "# Compare with Random Forest\n",
    "print(\"\\nXGBoost Performance:\")\n",
    "print(f\"MAE: {xgb_mae:.4f}\")\n",
    "print(f\"RMSE: {xgb_rmse:.4f}\")\n",
    "print(f\"R² Score: {xgb_r2:.4f}\")\n",
    "print(f\"Improvement over naive baseline: {xgb_improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Performance Metrics Plot ---\n",
    "# Combined Performance Metrics Plot\n",
    "metrics = {\n",
    "    'XGBoost MAE': xgb_mae,\n",
    "    'XGBoost RMSE': xgb_rmse,\n",
    "    'XGBoost R²': xgb_r2,\n",
    "    'RF MAE': mae,\n",
    "    'RF RMSE': rmse,\n",
    "    'RF R²': r2,\n",
    "    'Baseline MAE': naive_mae\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['#1f77b4', '#1f77b4', '#1f77b4', \n",
    "          '#ff7f0e', '#ff7f0e', '#ff7f0e',\n",
    "          '#d62728']\n",
    "bars = plt.bar(metrics.keys(), metrics.values(), color=colors)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Add legend\n",
    "import matplotlib.patches as mpatches\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color='#1f77b4', label='XGBoost'),\n",
    "    mpatches.Patch(color='#ff7f0e', label='Random Forest'),\n",
    "    mpatches.Patch(color='#d62728', label='Baseline')\n",
    "]\n",
    "plt.legend(handles=legend_patches)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance Visualization ---\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "plt.subplot(1, 2, 1)\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "sns.barplot(data=rf_feature_importance, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "plt.subplot(1, 2, 2)\n",
    "xgb_feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "sns.barplot(data=xgb_feature_importance, x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation with TimeSeriesSplit\n",
    "\n",
    "To ensure robust performance across different time periods, we applied **Time Series Cross-Validation** with 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit cross-validation for Random Forest\n",
    "# Define features and target\n",
    "features = [\n",
    "    'Day_of_Week', 'Month', 'Is_Weekend',\n",
    "    'Lag_1_Day', 'Lag_7_Day', 'Rolling_Avg_7',\n",
    "    'Sales_Drop_Flag', 'Consecutive_Zero_Sales', \n",
    "    'Category_Avg_Sales', 'Store_Percentile'\n",
    "]\n",
    "target = 'Units'\n",
    "\n",
    "# Sort the dataset by date\n",
    "merged_data = merged_data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Define TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_absolute_error',  # You can change to other metrics if needed\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit GridSearch to find best hyperparameters\n",
    "grid_search.fit(merged_data[features], merged_data[target])\n",
    "\n",
    "# Best estimator and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters Found:\", best_params)\n",
    "\n",
    "# Evaluate performance of the best model on each fold\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(merged_data)):\n",
    "    train_fold = merged_data.iloc[train_idx]\n",
    "    val_fold = merged_data.iloc[val_idx]\n",
    "    \n",
    "    model = RandomForestRegressor(**best_params, random_state=42)\n",
    "    model.fit(train_fold[features], train_fold[target])\n",
    "    \n",
    "    val_preds = model.predict(val_fold[features])\n",
    "    \n",
    "    fold_mae = mean_absolute_error(val_fold[target], val_preds)\n",
    "    fold_rmse = np.sqrt(mean_squared_error(val_fold[target], val_preds))\n",
    "    fold_r2 = r2_score(val_fold[target], val_preds)\n",
    "    \n",
    "    mae_scores.append(fold_mae)\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    r2_scores.append(fold_r2)\n",
    "    \n",
    "    print(f\"Fold {fold+1} - MAE: {fold_mae:.4f}, RMSE: {fold_rmse:.4f}, R²: {fold_r2:.4f}\")\n",
    "\n",
    "# Average metrics\n",
    "print(\"\\nCross-Validation Summary:\")\n",
    "print(f\"Avg MAE: {np.mean(mae_scores):.4f} ± {np.std(mae_scores):.4f}\")\n",
    "print(f\"Avg RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "print(f\"Avg R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "\n",
    "# ---- Final model training ----\n",
    "# Split the full dataset into train (80%) and test (20%) chronologically\n",
    "split_date = merged_data['Date'].quantile(0.8, interpolation='nearest')\n",
    "train_final = merged_data[merged_data['Date'] < split_date]\n",
    "test_final = merged_data[merged_data['Date'] >= split_date]\n",
    "\n",
    "# Retrain best model on full training set\n",
    "final_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "final_model.fit(train_final[features], train_final[target])\n",
    "print(\"\\n✅ Final model trained on full training set (80%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = [\n",
    "    'Day_of_Week', 'Month', 'Is_Weekend',\n",
    "    'Lag_1_Day', 'Lag_7_Day', 'Rolling_Avg_7',\n",
    "    'Sales_Drop_Flag', 'Consecutive_Zero_Sales', \n",
    "    'Category_Avg_Sales', 'Store_Percentile'\n",
    "]\n",
    "target = 'Units'\n",
    "\n",
    "# Sort the dataset by date\n",
    "merged_data = merged_data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Define TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define hyperparameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit GridSearch to find best hyperparameters\n",
    "grid_search.fit(merged_data[features], merged_data[target])\n",
    "\n",
    "# Best estimator and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters Found:\", best_params)\n",
    "\n",
    "# Evaluate performance of the best model on each fold\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(merged_data)):\n",
    "    train_fold = merged_data.iloc[train_idx]\n",
    "    val_fold = merged_data.iloc[val_idx]\n",
    "    \n",
    "    model = xgb.XGBRegressor(**best_params, random_state=42, objective='reg:squarederror')\n",
    "    model.fit(train_fold[features], train_fold[target])\n",
    "    \n",
    "    val_preds = model.predict(val_fold[features])\n",
    "    \n",
    "    fold_mae = mean_absolute_error(val_fold[target], val_preds)\n",
    "    fold_rmse = np.sqrt(mean_squared_error(val_fold[target], val_preds))\n",
    "    fold_r2 = r2_score(val_fold[target], val_preds)\n",
    "    \n",
    "    mae_scores.append(fold_mae)\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    r2_scores.append(fold_r2)\n",
    "    \n",
    "    print(f\"Fold {fold+1} - MAE: {fold_mae:.4f}, RMSE: {fold_rmse:.4f}, R²: {fold_r2:.4f}\")\n",
    "\n",
    "# Average metrics\n",
    "print(\"\\nCross-Validation Summary:\")\n",
    "print(f\"Avg MAE: {np.mean(mae_scores):.4f} ± {np.std(mae_scores):.4f}\")\n",
    "print(f\"Avg RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "print(f\"Avg R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "\n",
    "# ---- Final model training ----\n",
    "# Split the full dataset into train (80%) and test (20%) chronologically\n",
    "split_date = merged_data['Date'].quantile(0.8, interpolation='nearest')\n",
    "train_final = merged_data[merged_data['Date'] < split_date]\n",
    "test_final = merged_data[merged_data['Date'] >= split_date]\n",
    "\n",
    "# Retrain best model on full training set\n",
    "final_model = xgb.XGBRegressor(**best_params, random_state=42, objective='reg:squarederror')\n",
    "final_model.fit(train_final[features], train_final[target])\n",
    "print(\"\\n✅ Final model trained on full training set (80%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRF worst predictions\")  \n",
    "test['Predicted'] = predictions\n",
    "test['Error'] = abs(test['Units'] - test['Predicted'])\n",
    "print(test.nlargest(5, 'Error')[['Date','Store_ID','Product_ID','Units','Predicted','Error']])\n",
    "\n",
    "print(\"\\nXGBoost worst predictions\")\n",
    "test['Predicted'] = xgb_predictions\n",
    "test['Error'] = abs(test['Units'] - test['Predicted'])\n",
    "print(test.nlargest(5, 'Error')[['Date','Store_ID','Product_ID','Units','Predicted','Error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if errors cluster around zero-sales days\n",
    "sns.scatterplot(x=test['Units'], y=test['Predicted'])\n",
    "plt.plot([0, max(test['Units'])], [0, max(test['Units'])], 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Prediction Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(16, 8), dpi=120)\n",
    "\n",
    "# Sample 100 points evenly across the time period\n",
    "sample = test.iloc[np.linspace(0, len(test)-1, 100, dtype=int)].sort_values('Date')\n",
    "\n",
    "# Custom color palette\n",
    "palette = sns.color_palette(\"husl\", 2)\n",
    "actual_color = palette[0]  # Blue-green\n",
    "predicted_color = palette[1]  # Red-orange\n",
    "\n",
    "# Plot with enhanced styling\n",
    "ax.plot(sample['Date'], sample[target], \n",
    "        label='Actual Sales', \n",
    "        color=actual_color,\n",
    "        linewidth=3,\n",
    "        marker='o',\n",
    "        markersize=9,\n",
    "        markeredgecolor='white',\n",
    "        markeredgewidth=1.5,\n",
    "        alpha=0.9)\n",
    "\n",
    "ax.plot(sample['Date'], final_model.predict(sample[features]), \n",
    "        label='Predicted Sales', \n",
    "        color=predicted_color,\n",
    "        linewidth=2.5,\n",
    "        linestyle=(0, (5, 5)),  # Custom dashed pattern\n",
    "        marker='D',\n",
    "        markersize=7,\n",
    "        markeredgecolor='white',\n",
    "        markeredgewidth=1.2,\n",
    "        alpha=0.9)\n",
    "\n",
    "\n",
    "# Styling enhancements\n",
    "ax.set_title('Sales Prediction Performance\\nActual vs Predicted Values', \n",
    "             fontsize=16, pad=20, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12, labelpad=10)\n",
    "ax.set_ylabel('Sales Volume (Units)', fontsize=12, labelpad=10)\n",
    "\n",
    "# Configure grid and spines\n",
    "ax.grid(True, which='major', linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Date formatting\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d, %Y'))\n",
    "fig.autofmt_xdate(rotation=45, ha='center')\n",
    "\n",
    "# Modern legend\n",
    "legend = ax.legend(frameon=True, loc='upper right', \n",
    "                  facecolor='white', framealpha=0.95,\n",
    "                  edgecolor='#f0f0f0', borderpad=1)\n",
    "legend.get_frame().set_linewidth(0.8)\n",
    "\n",
    "# Add subtle background\n",
    "ax.set_facecolor('#f9f9f9')\n",
    "ax.grid(color='white', linewidth=1.5)\n",
    "\n",
    "ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make predictions with Random Forest if not already done\n",
    "if 'Predicted' not in test.columns:\n",
    "    predictions = final_model.predict(test[features])\n",
    "    test['Predicted'] = predictions\n",
    "\n",
    "# Choose 4 store IDs \n",
    "selected_stores = test['Store_ID'].unique()[:4]  # Changed to 4 stores to fill all subplots\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot for each selected store\n",
    "for i, store_id in enumerate(selected_stores):\n",
    "    store_data = test[test['Store_ID'] == store_id].sort_values('Date')\n",
    "\n",
    "    axes[i].plot(store_data['Date'], store_data['Units'], label='Actual', marker='o')\n",
    "    axes[i].plot(store_data['Date'], store_data['Predicted'], label='RF Predicted', marker='x')\n",
    "    axes[i].set_title(f'Store ID: {store_id}')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Units Sold')\n",
    "    axes[i].legend()\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make predictions if not already done\n",
    "if 'Predicted' not in test.columns:\n",
    "    predictions = final_model.predict(test[features])\n",
    "    test['Predicted'] = predictions\n",
    "\n",
    "# Choose 4 Product IDs to visualize\n",
    "selected_products = test['Product_ID'].unique()[:4]  # Adjust number as needed\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot for each selected product\n",
    "for i, product_id in enumerate(selected_products):\n",
    "    product_data = test[test['Product_ID'] == product_id].sort_values('Date')\n",
    "\n",
    "    axes[i].plot(product_data['Date'], product_data['Units'], label='Actual', marker='o')\n",
    "    axes[i].plot(product_data['Date'], product_data['Predicted'], label='RF Predicted', marker='x')\n",
    "    axes[i].set_title(f'Product ID: {product_id}')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Units Sold')\n",
    "    axes[i].legend()\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict and Plot for Top 3 Products in Top Store ---\n",
    "# Find the top store by total sales\n",
    "best_store_id = merged_data.groupby('Store_ID')['sales'].sum().idxmax()\n",
    "\n",
    "# Find top 3 products in that store by sales volume\n",
    "top_products = (\n",
    "    merged_data[merged_data['Store_ID'] == best_store_id]\n",
    "    .groupby('Product_ID')['Units'].sum()\n",
    "    .nlargest(3)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "# Plot predictions for each top product in the best store\n",
    "for product_id in top_products:\n",
    "    filtered_test = test[(test['Store_ID'] == best_store_id) & (test['Product_ID'] == product_id)].copy()\n",
    "    if filtered_test.empty:\n",
    "        continue\n",
    "    filtered_test['Predicted'] = final_model.predict(filtered_test[features])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(filtered_test['Date'], filtered_test['Units'], label='Actual Units', marker='o')\n",
    "    plt.plot(filtered_test['Date'], filtered_test['Predicted'], label='Predicted Units', marker='x')\n",
    "    plt.title(f'Top Store ID: {best_store_id} | Product ID: {product_id} — Actual vs Predicted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Units Sold')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store prediction rows\n",
    "prediction_rows = []\n",
    "\n",
    "# Normalize dates to remove time (if not already done)\n",
    "test['Date'] = pd.to_datetime(test['Date']).dt.normalize()\n",
    "\n",
    "# --- Predict and Collect Data for Top 3 Products in Top Store ---\n",
    "best_store_id = merged_data.groupby('Store_ID')['sales'].sum().idxmax()\n",
    "\n",
    "top_products = (\n",
    "    merged_data[merged_data['Store_ID'] == best_store_id]\n",
    "    .groupby('Product_ID')['Units'].sum()\n",
    "    .nlargest(3)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "# Collect and aggregate predictions\n",
    "for product_id in top_products:\n",
    "    filtered_test = test[(test['Store_ID'] == best_store_id) & (test['Product_ID'] == product_id)].copy()\n",
    "    if filtered_test.empty:\n",
    "        continue\n",
    "    \n",
    "    filtered_test['Predicted'] = final_model.predict(filtered_test[features])\n",
    "    filtered_test['Predicted'] = np.round(filtered_test['Predicted']).astype(int)\n",
    "\n",
    "    # Group by date to get total predicted units per day\n",
    "    daily_pred = (\n",
    "        filtered_test.groupby('Date')['Predicted']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={'Predicted': 'units'})\n",
    "    )\n",
    "    daily_pred['store_id'] = best_store_id\n",
    "    daily_pred['product_id'] = product_id\n",
    "\n",
    "    prediction_rows.append(daily_pred)\n",
    "\n",
    "# Combine all predictions into one DataFrame\n",
    "predictions_df = pd.concat(prediction_rows, ignore_index=True)\n",
    "\n",
    "# Add incremental ID column\n",
    "predictions_df.insert(0, 'id', range(1, len(predictions_df) + 1))\n",
    "\n",
    "# Save to CSV\n",
    "predictions_df.to_csv('predicted_units.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make sure 'Date' is normalized (remove time component)\n",
    "test['Date'] = pd.to_datetime(test['Date']).dt.normalize()\n",
    "\n",
    "# Predict and attach to test set\n",
    "if 'Predicted' not in test.columns:\n",
    "    test['Predicted'] = final_model.predict(test[features])\n",
    "    test['Predicted'] = np.round(test['Predicted']).astype(int)\n",
    "\n",
    "# Group by store, product, and date, then round the daily total\n",
    "daily_predictions = (\n",
    "    test.groupby(['Store_ID', 'Product_ID', 'Date'])['Predicted']\n",
    "    .sum()\n",
    "    .round()\n",
    "    .astype(int)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns to match Supabase table\n",
    "daily_predictions.rename(columns={\n",
    "    'Store_ID': 'store_id',\n",
    "    'Product_ID': 'product_id',\n",
    "    'Date': 'date',\n",
    "    'Predicted': 'units'  # if still named Predicted (just in case)\n",
    "}, inplace=True)\n",
    "\n",
    "# Add incremental ID column\n",
    "daily_predictions.insert(0, 'id', range(1, len(daily_predictions) + 1))\n",
    "\n",
    "# Save to CSV\n",
    "daily_predictions.to_csv('all_stores_products_predicted_units.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both models\n",
    "joblib.dump(final_model, 'random_forest_model.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
